
\section{Results and discussion}
4) Results and discussion, including a comparison between the results of the 
replication study and the original study and a list of the limitations 
encountered during the replication.
\subsection{RQ1: What source code properties characterize defective 
infrastructure as code scripts?}
After completing the \emph{Mann-Whitney U} test and \emph{Cliff's Delta} test
we can identify which properties have a $p-value < 0.05$ for all datasets (i.e. 
Attribute, Command, Ensure, File, File mode, Hard coded string, Include, Lines 
of code, Require and SSH Key). Surprisingly, we don't quite get the same results 
as the paper. In our case, the \emph{Comment} property doesn't have a $p-value < 0.05$
Actually, in the paper either but it's in bold in Table 8. Maybe it's a simple 
mistake. You can see these results in Table \ref{table:rq1-mirantis} for the 
Mirantis dataset, in Table \ref{table:rq1-mozilla} for the Mozilla dataset and 
in Table \ref{table:rq1-wikimedia} for the Wikimedia dataset.


\begin{table}[h]
    \caption{Validation of identified source code properties for Mirantis}
    \label{table:rq1-mirantis}
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
        Property & $p-value$ & $delta$ \\ \hline
        Attribute & $<0.001$ & 0.47 \\ \hline
        Command & 0.005 & 0.24 \\ \hline
        Comment & $<0.001$ & 0.37 \\ \hline
        Ensure & $<0.001$ & 0.38 \\ \hline
        File & $<0.001$ & 0.36 \\ \hline
        File mode & $<0.001$ & 0.41 \\ \hline
        Hard coded string & $<0.001$ & 0.55 \\ \hline
        Include & $<0.001$ & 0.33 \\ \hline
        Lines of code & $<0.001$ & 0.45 \\ \hline
        Require & $<0.001$ & 0.36 \\ \hline
        SSH KEY & $<0.001$ & 0.39 \\ \hline
        URL & 0.009 & 0.22 \\ \hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{Validation of identified source code properties for Mozilla}
    \label{table:rq1-mozilla}
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
        Property & $p-value$ & $delta$ \\ \hline
        Attribute & $<0.001$ & 0.40 \\ \hline
        Command & $<0.001$ & 0.18 \\ \hline
        Comment & 0.58 & 0.03 \\ \hline
        Ensure & $<0.001$ & 0.09 \\ \hline
        File & $<0.001$ & 0.18 \\ \hline
        File mode & $<0.001$ & 0.24 \\ \hline
        Hard coded string & $<0.001$ & 0.40 \\ \hline
        Include & $<0.001$ & 0.31 \\ \hline
        Lines of code & $<0.001$ & 0.50 \\ \hline
        Require & $<0.001$ & 0.19 \\ \hline
        SSH KEY & $<0.001$ & 0.24 \\ \hline
        URL & 0.081 & 0.08 \\ \hline
    \end{tabular}
\end{table}


\begin{table}[h]
    \caption{Validation of identified source code properties for Wikimedia}
    \label{table:rq1-wikimedia}
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
        Property & $p-value$ & $delta$ \\ \hline
        Attribute & $<0.001$ & 0.47 \\ \hline
        Command & 0.008 & 0.18 \\ \hline
        Comment & $<0.001$ & 0.22 \\ \hline
        Ensure & $<0.001$ & 0.29 \\ \hline
        File & $<0.001$ & 0.31 \\ \hline
        File mode & $<0.001$ & 0.24 \\ \hline
        Hard coded string & $<0.001$ & 0.55 \\ \hline
        Include & $<0.001$ & 0.37 \\ \hline
        Lines of code & $<0.001$ & 0.51 \\ \hline
        Require & $<0.001$ & 0.32 \\ \hline
        SSH KEY & $<0.001$ & 0.24 \\ \hline
        URL & 0.011 & 0.17 \\ \hline
    \end{tabular}
\end{table}

\subsection{RQ3: How can we construct defect prediction models for 
infrastructure as code scripts using the identified source code properties?}
As mentionned in the previous section, we only used the principle components 
that accounted for at least 95\% of the total variance. We can see in Table
\ref{table:pca} that only one or two principle components account for 95\% of 
the total variance depending on the dataset. The number of principle components
for each dataset corresponds to the ones in the paper.\\

Since the paper doesn't specify the different parameters for the models, 
it would be difficult to obtain exactly the same results. Nevertheless, we 
obtained results that are very similar that bring to the same conclusions.
The results from the cross-validation for each model can be found in Table 
\ref{table:mirantis-models} for the Mirantis dataset, in Table \ref{table:mozilla-models}
for the Mozilla dataset, in Table \ref{table:openstack-models} for the Openstack
dataset and in Table \ref{table:wikimedia-models} for the Wikimedia dataset. 
We haven't included the results from the actual paper, since it's publicly 
available.


\begin{table}[h]
  \caption{Number of principle components for the models}
  \label{table:pca}
    \centering
    \begin{tabular}{|l|c|}
    \hline
        Dataset   & Number of components \\ \hline
        Mirantis  & 1 \\ \hline
        Mozilla   & 1 \\ \hline
        Openstack & 2 \\ \hline
        Wikimedia & 2 \\ \hline
    \end{tabular}
\end{table}




\begin{table}[h]
  \caption{Cross-validation results for Mirantis}
  \label{table:mirantis-models}
    \centering
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
        ~ & RF & NB & LR & KNN & CART \\ \hline
        AUC & 0.701661 & 0.714252 & 0.750981 & 0.693334 & 0.659597 \\ \hline
        Recall & 0.707425 & 0.407360 & 0.649691 & 0.672546 & 0.707425 \\ \hline
        Precision & 0.701199 & 0.846909 & 0.798322 & 0.667389 & 0.701199 \\ \hline
        F1-measure  & 0.695896 & 0.541781 & 0.708236 & 0.663964 & 0.698448 \\ \hline
    \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Cross-validation results for Mozilla}
  \label{table:mozilla-models}
    \centering
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
        ~ & RF & NB & LR & KNN & CART \\ \hline
        AUC & 0.731664 & 0.699599 & 0.756323 & 0.713161 & 0.691230 \\ \hline
        Recall & 0.649017 & 0.392519 & 0.565923 & 0.619417 & 0.627106 \\ \hline
        Precision & 0.642651 & 0.831862 & 0.706600 & 0.604170 & 0.642550 \\ \hline
        F1-measure & 0.645764 & 0.532261 & 0.626990 & 0.608864 & 0.633393 \\ \hline
    \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Cross-validation results for Openstack}
  \label{table:openstack-models}
    \centering
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
        ~ & RF & NB & LR & KNN & CART \\ \hline
        AUC & 0.647741 & 0.694343 & 0.659972 & 0.659195 & 0.574832 \\ \hline
        Recall & 0.667616 & 0.368902 & 0.731321 & 0.687022 & 0.660176 \\ \hline
        Precision & 0.653112 & 0.847009 & 0.643218 & 0.661449 & 0.655685 \\ \hline
        F1-measure & 0.660440 & 0.512676 & 0.682243 & 0.673287 & 0.657360 \\ \hline
    \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Cross-validation results for Wikimedia}
  \label{table:wikimedia-models}
    \centering
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
        ~ & RF & NB & LR & KNN & CART \\ \hline
        AUC & 0.664721 & 0.709438 & 0.736270 & 0.699140 & 0.583164 \\ \hline
        Recall & 0.591171 & 0.366128 & 0.586200 & 0.627349 & 0.587493 \\ \hline
        Precision & 0.664007 & 0.885945 & 0.774041 & 0.732415 & 0.666620 \\ \hline
        F1-measure & 0.628276 & 0.515651 & 0.663515 & 0.673132 & 0.623400 \\ \hline
    \end{tabular}
\end{table}
